# ArgoAI: LLM-Powered GitHub Code Review Bot

## Project Overview

### Concept
ArgoAI is an intelligent code review bot that automatically analyzes pull requests on GitHub using Large Language Models (LLMs) to provide meaningful feedback, catch potential issues, and suggest improvements. Built on Cloudflare Workers for global edge deployment, the bot aims to augment human code reviews by providing consistent, immediate, and comprehensive analysis with minimal latency.

### Key Features
- **Automated PR Analysis**: Triggers on new pull requests and updates via GitHub webhooks
- **Context-Aware Reviews**: Understands project structure and coding patterns
- **Multi-Language Support**: Reviews code in various programming languages
- **Configurable Rules**: Customizable review criteria via environment variables
- **Incremental Reviews**: Analyzes only changed files/lines for efficiency
- **Smart Comments**: Posts actionable, non-redundant feedback
- **Review Summary**: Provides high-level PR assessment
- **Edge Deployment**: Zero cold starts with global distribution via Cloudflare Workers
- **Real-time Processing**: Sub-second webhook processing at the edge

### Value Proposition
- Reduces code review turnaround time to seconds
- Catches common issues before human review
- Ensures consistent code quality standards across all time zones
- Educates developers through detailed feedback
- Scales review capacity for growing teams without infrastructure overhead
- Global performance with edge computing

## GitHub App Name Reservation: "ArgoAI"

### Important: GitHub App Name Registration Process

GitHub Apps do not have a separate name reservation system. To secure the name "ArgoAI" for your bot:

1. **Act Quickly**: GitHub App names are first-come, first-served. The name "ArgoAI" will be available until someone creates an app with that name.

2. **Registration Steps**:
   - Go to your GitHub Settings â†’ Developer settings â†’ GitHub Apps
   - Click "New GitHub App"
   - Enter "ArgoAI" as the GitHub App name
   - Fill in the required fields (you can update these later):
     - Homepage URL: `https://argoai.dev` (can be a placeholder)
     - Webhook URL: `https://api.argoai.dev/webhooks/github` (can be updated later)
     - Webhook secret: Generate a secure random string
     - Permissions: Start with minimal permissions (can be expanded later)

3. **Name Requirements**:
   - Maximum 34 characters (ArgoAI is only 6 characters âœ“)
   - Must be unique across all GitHub
   - Will be displayed as lowercase with special characters replaced

4. **Important Notes**:
   - Once registered, the name is yours and cannot be taken by others
   - You can modify all settings after registration
   - GitHub prohibits name squatting - you must actively use the app
   - The app doesn't need to be functional immediately after registration

5. **Fallback Names** (if ArgoAI is taken):
   - ArgoAI-Bot
   - ArgoCodeReview
   - ArgoReviewer
   - Argo-AI
   - ArgoAI-CR

### Action Items for Name Registration
1. **Immediately**: Register the GitHub App with name "ArgoAI"
2. **Within 24 hours**: Set up basic webhook endpoint
3. **Within 1 week**: Deploy initial version to maintain active status

## System Architecture

### High-Level Architecture - Cloudflare Workers Edge-First Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                          â”‚     â”‚                     â”‚
â”‚  GitHub Events  â”œâ”€â”€â”€â”€â–ºâ”‚  Cloudflare Workers      â”œâ”€â”€â”€â”€â–ºâ”‚   LLM Provider API  â”‚
â”‚   (Webhooks)    â”‚     â”‚  (Global Edge Network)   â”‚     â”‚  (OpenAI/Claude)    â”‚
â”‚                 â”‚     â”‚                          â”‚     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                                    â–²
                                 â”‚                                    â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
                        â”‚                          â”‚                 â”‚
                        â”‚  Worker Entry Point      â”‚                 â”‚
                        â”‚  - Webhook validation    â”‚                 â”‚
                        â”‚  - Request routing       â”‚                 â”‚
                        â”‚  - Rate limiting         â”‚                 â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                                 â”‚                                    â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
                        â”‚                          â”‚                 â”‚
                        â”‚  Review Worker           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  - Fetch PR diff         â”‚
                        â”‚  - Build LLM prompt      â”‚
                        â”‚  - Process response      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                             â”‚                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚                  â”‚          â”‚              â”‚  â”‚              â”‚
         â”‚  Workers KV      â”‚          â”‚ Workers D1   â”‚  â”‚ GitHub API   â”‚
         â”‚  - Cache results â”‚          â”‚ - PR states  â”‚  â”‚ - Comments   â”‚
         â”‚  - Rate limits   â”‚          â”‚ - Audit logs â”‚  â”‚ - Reactions  â”‚
         â”‚                  â”‚          â”‚              â”‚  â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Cloudflare Global Network                        â”‚
â”‚  â€¢ 300+ cities worldwide  â€¢ <50ms latency  â€¢ Zero cold starts         â”‚
â”‚  â€¢ Automatic scaling      â€¢ DDoS protection â€¢ Built-in analytics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components - Cloudflare Workers Implementation

#### 1. **Edge Webhook Handler (Main Worker)**
```typescript
// Deployed globally across Cloudflare's network
export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    // Validate GitHub webhook signature
    // Route to appropriate handler
    // Return immediate response to GitHub
  }
}
```
- Validates webhook signatures using Web Crypto API
- Filters relevant events (PR opened, synchronized, etc.)
- Triggers review processing via Service Bindings
- Returns response in <50ms globally

#### 2. **Review Processing Worker**
```typescript
// Handles the actual review logic
export class ReviewWorker {
  async processReview(pr: PullRequest): Promise<void> {
    // Fetch PR diff from GitHub
    // Build context-aware LLM prompt
    // Stream response from LLM
    // Post comments back to GitHub
  }
}
```
- Fetches PR diff and file contents via GitHub API
- Constructs intelligent LLM prompts with context
- Handles streaming responses from LLM providers
- Posts structured comments to GitHub

#### 3. **Lightweight Storage Strategy**
- **Workers KV (Primary)**: 
  - Review cache (1 hour TTL) - only actionable comments
  - Rate limit counters (60s TTL)
  - Deduplication keys (24 hour TTL)
  - Active PR tracking
- **No Persistent Logs**: 
  - Audit logs streamed to stdout (captured by Logpush if needed)
  - No long-term PR history storage
  - Metrics aggregated in-memory and flushed periodically
- **Minimal D1 Usage (Optional)**:
  - Configuration overrides only
  - No review history to keep data lightweight

#### 4. **Queue System (Cloudflare Queues)**

**Why Cloudflare Queues over alternatives:**
- **Native Integration**: First-class support in Workers
- **Zero Configuration**: No separate infrastructure
- **At-least-once delivery**: Guaranteed message processing
- **Automatic retries**: With exponential backoff
- **Dead letter queues**: For handling persistent failures
- **Cost effective**: 1M messages/month free

**Alternatives considered:**
- **Azure Service Bus**: Requires external connectivity, higher latency
- **Apache Kafka**: Overkill for this use case, complex operations
- **Redis Streams**: Need separate Redis instance, not edge-native
- **AWS SQS**: Cross-cloud complexity, egress costs
- **RabbitMQ**: Self-hosted overhead, not serverless

```typescript
// Producer in webhook handler
await env.REVIEW_QUEUE.send({
  prNumber: pr.number,
  repo: pr.repository.full_name,
  action: pr.action,
  installationId: pr.installation.id,
  timestamp: Date.now()
});

// Consumer configuration in wrangler.toml
[env.production.queues]
producers = [{ binding = "REVIEW_QUEUE", queue = "argoai-reviews" }]
consumers = [{
  queue = "argoai-reviews",
  max_batch_size = 10,
  max_batch_timeout = 30,
  max_retries = 3,
  dead_letter_queue = "argoai-dlq"
}]

// Consumer implementation
export default {
  async queue(batch: MessageBatch<ReviewTask>, env: Env): Promise<void> {
    // Process messages in parallel for efficiency
    await Promise.all(
      batch.messages.map(async (message) => {
        try {
          await processReview(message.body, env);
          message.ack(); // Acknowledge successful processing
        } catch (error) {
          console.error(`Failed to process review: ${error}`);
          message.retry(); // Retry with exponential backoff
        }
      })
    );
  }
}
```
- Native Cloudflare Queues integration
- Automatic retries with exponential backoff
- Dead letter queue for persistent failures
- Batch processing for efficiency (up to 10 messages)
- At-least-once delivery guarantee

### Technology Stack - Cloudflare Workers Native

#### Core Platform
- **Runtime**: Cloudflare Workers (V8 Isolates)
- **Language**: TypeScript with Workers Types
- **API Framework**: Hono (optimized for Workers)
- **Queue**: Cloudflare Queues
- **Storage**: 
  - Workers KV (key-value cache)
  - Workers D1 (SQLite database)
  - R2 (object storage for logs)
- **LLM Integration**: GitHub Models API (free tier with GitHub token)

#### Development & Deployment
- **CLI**: Wrangler 3.x
- **Local Dev**: Miniflare 3
- **Testing**: Vitest with Workers environment
- **CI/CD**: GitHub Actions with Wrangler
- **Monitoring**: Workers Analytics + Logpush
- **Secrets**: Workers Secrets (encrypted env vars)

## Platform Choice: Cloudflare Workers

We've chosen Cloudflare Workers as the primary platform for ArgoAI due to its:
- **Zero cold starts** - Critical for GitHub webhook timeouts
- **Global edge deployment** - Sub-50ms response times worldwide
- **Native queuing** - Cloudflare Queues for async processing
- **Cost efficiency** - Often fits within free tier ($0-50/month)
- **Minimal ops overhead** - No servers to manage

For alternative hosting options and detailed comparisons, see [hosting-alternatives.md](./hosting-alternatives.md).

## Environment Configuration - Cloudflare Workers

### Cloudflare Workers Configuration

#### wrangler.toml - Main Configuration
```toml
name = "argoai"
main = "src/index.ts"
compatibility_date = "2024-01-01"
workers_dev = false

# Production environment
[env.production]
routes = [
  { pattern = "api.argoai.dev/*", zone_name = "argoai.dev" }
]

# KV Namespaces
kv_namespaces = [
  { binding = "CACHE", id = "your-cache-kv-id" },
  { binding = "RATE_LIMITS", id = "your-ratelimit-kv-id" },
  { binding = "CONFIG", id = "your-config-kv-id" }
]

# D1 Database (optional - only for config storage)
# d1_databases = [
#   { binding = "DB", database_name = "argoai-config", database_id = "your-d1-id" }
# ]

# Queues
[env.production.queues]
producers = [
  { binding = "REVIEW_QUEUE", queue = "argoai-reviews" }
]
consumers = [
  { queue = "argoai-reviews", max_batch_size = 10, max_batch_timeout = 30 }
]

# R2 Buckets (optional - use Logpush instead for logs)
# r2_buckets = [
#   { binding = "LOGS", bucket_name = "argoai-logs" }
# ]

# Service Bindings (for multi-worker architecture)
services = [
  { binding = "ANALYTICS", service = "argoai-analytics" }
]

# Environment Variables (non-secret)
[env.production.vars]
GITHUB_APP_ID = "123456"
GITHUB_MODEL = "gpt-4o-mini"  # Free tier model from GitHub Models
REVIEW_ENABLED_EVENTS = "pull_request.opened,pull_request.synchronize"
LOG_LEVEL = "info"
MAX_FILES_PER_REVIEW = "20"  # Limit to keep reviews focused
```

#### Secrets Configuration (via wrangler secret)
```bash
# Set secrets securely (not in wrangler.toml)
wrangler secret put GITHUB_APP_PRIVATE_KEY --env production
wrangler secret put GITHUB_WEBHOOK_SECRET --env production
wrangler secret put GITHUB_TOKEN --env production  # For GitHub Models API

# Optional secrets for advanced features
wrangler secret put SLACK_WEBHOOK_URL --env production
wrangler secret put SENTRY_DSN --env production
```

#### Type Definitions (src/types.ts)
```typescript
export interface Env {
  // Secrets
  GITHUB_APP_PRIVATE_KEY: string
  GITHUB_WEBHOOK_SECRET: string
  GITHUB_TOKEN: string  // For GitHub Models API access
  
  // KV Namespaces
  CACHE: KVNamespace
  RATE_LIMITS: KVNamespace
  CONFIG: KVNamespace
  
  // D1 Database (optional - for config only)
  DB?: D1Database
  
  // Queues
  REVIEW_QUEUE: Queue
  
  // Service Bindings (optional)
  ANALYTICS?: Fetcher
  
  // Environment Variables
  GITHUB_APP_ID: string
  GITHUB_MODEL: string  // e.g., 'gpt-4o-mini', 'Phi-3-small-8k-instruct'
  REVIEW_ENABLED_EVENTS: string
  LOG_LEVEL: 'debug' | 'info' | 'warn' | 'error'
}
```

### Dynamic Configuration via Workers KV

Store dynamic configuration in KV for runtime updates without redeployment:

```typescript
// Set configuration via KV
await env.CONFIG.put('review:rules', JSON.stringify({
  maxFilesPerPR: 50,
  maxLinesPerFile: 1000,
  filePatterns: ['**/*.{js,ts,py,go,java}'],
  ignorePatterns: ['**/node_modules/**', '**/vendor/**'],
  reviewPromptTemplate: 'custom-prompt-v2'
}))

// Read configuration in worker
const config = await env.CONFIG.get('review:rules', 'json')
```

### Local Development Configuration

#### .dev.vars - Local Development Secrets
```bash
# Create .dev.vars file (gitignored)
GITHUB_APP_PRIVATE_KEY=-----BEGIN RSA PRIVATE KEY-----...
GITHUB_WEBHOOK_SECRET=dev-webhook-secret
GITHUB_TOKEN=ghp_... # Your GitHub personal access token with models:read scope
```

#### Local Testing with Miniflare
```bash
# Run locally with Miniflare
wrangler dev --local --persist

# Run with remote resources
wrangler dev --env production
```

## Detailed Technical Implementation

### 1. GitHub Webhook Handler Implementation

```typescript
// src/github/webhook.ts
import { createHmac } from 'node:crypto'

export async function verifyWebhookSignature(
  request: Request,
  secret: string
): Promise<boolean> {
  const signature = request.headers.get('X-Hub-Signature-256')
  if (!signature) return false
  
  const body = await request.text()
  const expectedSignature = `sha256=${createHmac('sha256', secret)
    .update(body)
    .digest('hex')}`
  
  return signature === expectedSignature
}

export async function handlePullRequestEvent(
  payload: PullRequestEvent,
  env: Env
): Promise<void> {
  const { action, pull_request, repository } = payload
  
  // Check if we should process this event
  const enabledEvents = env.REVIEW_ENABLED_EVENTS.split(',')
  if (!enabledEvents.includes(`pull_request.${action}`)) {
    return
  }
  
  // Queue the review task
  await env.REVIEW_QUEUE.send({
    type: 'review',
    timestamp: Date.now(),
    data: {
      action,
      prNumber: pull_request.number,
      repo: repository.full_name,
      sha: pull_request.head.sha,
      diffUrl: pull_request.diff_url,
      filesUrl: pull_request.url + '/files'
    }
  })
}
```

### 2. LLM Integration with GitHub Models

```typescript
// src/llm/reviewer.ts
export class CodeReviewer {
  constructor(private env: Env) {}
  
  async reviewPullRequest(pr: PullRequestData): Promise<ReviewResult> {
    // Check cache first
    const cacheKey = `review:${pr.repo}:${pr.sha}`
    const cached = await this.env.CACHE.get(cacheKey, 'json')
    if (cached) return cached as ReviewResult
    
    // Fetch PR diff
    const diff = await this.fetchPRDiff(pr)
    
    // Build prompt
    const prompt = this.buildReviewPrompt(diff, pr)
    
    // Call GitHub Models API
    const review = await this.callGitHubModels(prompt)
    
    // Cache result (lightweight - only actionable feedback)
    await this.env.CACHE.put(cacheKey, JSON.stringify({
      summary: review.summary,
      comments: review.comments.filter(c => c.severity !== 'info')
    }), {
      expirationTtl: 3600 // 1 hour
    })
    
    return review
  }
  
  private async callGitHubModels(prompt: string): Promise<ReviewResult> {
    // GitHub Models endpoint - uses GitHub token for auth
    const response = await fetch('https://models.inference.ai.azure.com/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.env.GITHUB_TOKEN}`,
        'Content-Type': 'application/json',
        'X-GitHub-Api-Version': '2022-11-28'
      },
      body: JSON.stringify({
        model: this.env.GITHUB_MODEL || 'gpt-4o-mini', // Free tier model
        messages: [
          {
            role: 'system',
            content: `You are ArgoAI, an expert code reviewer. Focus on:
              - Security vulnerabilities
              - Performance issues
              - Code quality and best practices
              - Potential bugs
              Be concise and actionable. Skip style issues unless severe.`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        stream: true,
        temperature: 0.3,
        max_tokens: 2000 // Optimize token usage
      })
    })
    
    if (!response.ok) {
      // Fallback to simpler model if rate limited
      if (response.status === 429) {
        return this.callGitHubModelsSimple(prompt)
      }
      throw new Error(`GitHub Models API error: ${response.status}`)
    }
    
    // Process streaming response
    const reader = response.body?.getReader()
    let fullResponse = ''
    
    while (true) {
      const { done, value } = await reader!.read()
      if (done) break
      
      const text = new TextDecoder().decode(value)
      // Parse SSE format and accumulate response
      fullResponse += this.parseSSE(text)
    }
    
    return this.parseReviewResponse(fullResponse)
  }
  
  private async callGitHubModelsSimple(prompt: string): Promise<ReviewResult> {
    // Fallback to non-streaming for rate limit handling
    const response = await fetch('https://models.inference.ai.azure.com/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.env.GITHUB_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'Phi-3-small-8k-instruct', // Lighter model
        messages: [{
          role: 'user',
          content: `Review this code change briefly: ${prompt.substring(0, 1000)}`
        }],
        stream: false,
        max_tokens: 500
      })
    })
    
    const data = await response.json()
    return this.parseSimpleResponse(data.choices[0].message.content)
  }
}
```

### 3. Smart Comment Posting

```typescript
// src/github/commenter.ts
export class GitHubCommenter {
  private octokit: Octokit
  
  constructor(private env: Env) {
    const app = new App({
      appId: env.GITHUB_APP_ID,
      privateKey: env.GITHUB_APP_PRIVATE_KEY
    })
    this.octokit = app.getInstallationOctokit(installationId)
  }
  
  async postReview(
    review: ReviewResult,
    pr: PullRequestData
  ): Promise<void> {
    // Group comments by file
    const commentsByFile = this.groupCommentsByFile(review.comments)
    
    // Create review with comments
    await this.octokit.pulls.createReview({
      owner: pr.owner,
      repo: pr.repo,
      pull_number: pr.prNumber,
      commit_id: pr.sha,
      body: review.summary,
      event: 'COMMENT',
      comments: commentsByFile.map(({ path, line, comment }) => ({
        path,
        line,
        body: this.formatComment(comment)
      }))
    })
    
    // Post summary as PR comment
    await this.octokit.issues.createComment({
      owner: pr.owner,
      repo: pr.repo,
      issue_number: pr.prNumber,
      body: this.formatSummary(review)
    })
  }
  
  private formatComment(comment: ReviewComment): string {
    return `ğŸ¤– **ArgoAI Review**\n\n${comment.message}\n\n` +
           `**Severity:** ${comment.severity}\n` +
           `**Category:** ${comment.category}`
  }
}
```

### 4. Rate Limiting Implementation

```typescript
// src/middleware/rateLimit.ts
export async function rateLimitMiddleware(
  c: Context,
  next: Next
): Promise<Response> {
  const clientId = c.req.header('X-GitHub-Delivery') || 'unknown'
  const key = `rate:${clientId}`
  
  // Get current count
  const current = await c.env.RATE_LIMITS.get(key)
  const count = current ? parseInt(current) : 0
  
  // Check limit
  const limit = 60 // per minute
  if (count >= limit) {
    return c.json({ error: 'Rate limit exceeded' }, 429)
  }
  
  // Increment with TTL
  await c.env.RATE_LIMITS.put(key, String(count + 1), {
    expirationTtl: 60
  })
  
  return next()
}
```

### 5. Minimal Storage Configuration

```typescript
// src/storage/config.ts
// D1 is optional - only use if you need persistent config overrides
// Most configuration should come from KV or environment variables

export class ConfigStore {
  constructor(private env: Env) {}
  
  async getRepoConfig(repo: string): Promise<RepoConfig | null> {
    // First check KV for quick access
    const cached = await this.env.CONFIG.get(`repo:${repo}`, 'json')
    if (cached) return cached as RepoConfig
    
    // Optionally check D1 if configured
    if (this.env.DB) {
      const result = await this.env.DB.prepare(
        'SELECT config FROM repo_configs WHERE repo_name = ?'
      ).bind(repo).first()
      
      if (result) {
        const config = JSON.parse(result.config as string)
        // Cache in KV for next time
        await this.env.CONFIG.put(`repo:${repo}`, JSON.stringify(config), {
          expirationTtl: 3600
        })
        return config
      }
    }
    
    return null
  }
}

// Lightweight metrics - no persistent storage
export class MetricsCollector {
  private metrics = new Map<string, number>()
  
  increment(key: string): void {
    this.metrics.set(key, (this.metrics.get(key) || 0) + 1)
  }
  
  async flush(env: Env): Promise<void> {
    // Log metrics to stdout for Logpush collection
    console.log(JSON.stringify({
      type: 'metrics',
      timestamp: Date.now(),
      data: Object.fromEntries(this.metrics)
    }))
    this.metrics.clear()
  }
}
```

### 6. Lightweight Monitoring

```typescript
// src/monitoring/logger.ts
export class Logger {
  constructor(private env: Env) {}
  
  logReview(pr: PullRequestData, result: ReviewResult, duration: number): void {
    // Structured logging for Cloudflare Logpush
    console.log(JSON.stringify({
      type: 'review',
      timestamp: Date.now(),
      repo: pr.repo,
      pr_number: pr.prNumber,
      duration_ms: duration,
      comments_count: result.comments.length,
      cache_hit: result.fromCache || false,
      model: this.env.GITHUB_MODEL,
      // No PII - just metrics
    }))
  }
  
  logError(error: Error, context: any): void {
    console.error(JSON.stringify({
      type: 'error',
      timestamp: Date.now(),
      error: error.message,
      stack: error.stack,
      context
    }))
  }
}

// Use Cloudflare Analytics for aggregated metrics
export function setupAnalytics(env: Env): void {
  // Cloudflare Workers Analytics automatically tracks:
  // - Request count
  // - Error rates
  // - CPU time
  // - Response times
  // No additional setup needed!
}
```

## Implementation Best Practices

### Security
1. **Webhook Validation**: Always verify GitHub webhook signatures
2. **API Key Management**: Use secure vaults (Azure Key Vault, AWS Secrets Manager)
3. **Rate Limiting**: Implement per-repo and per-org limits
4. **Input Sanitization**: Sanitize all LLM outputs before posting
5. **Least Privilege**: Use minimal GitHub App permissions

### Performance
1. **Async Processing**: Use non-blocking I/O throughout
2. **Batch Operations**: Group API calls where possible
3. **Caching Strategy**: Cache LLM responses for identical code
4. **Connection Pooling**: Reuse HTTP connections
5. **Timeout Handling**: Set aggressive timeouts for all external calls

### Reliability
1. **Idempotency**: Ensure reviews can be safely retried
2. **Circuit Breakers**: Protect against downstream failures
3. **Health Checks**: Implement comprehensive health endpoints
4. **Graceful Degradation**: Continue partial functionality during outages
5. **Error Budgets**: Define acceptable error rates

### Observability
1. **Structured Logging**: Use JSON logs with correlation IDs
2. **Distributed Tracing**: Track requests across services
3. **Custom Metrics**: Track review times, queue depths, cache hits
4. **Alerting**: Set up proactive alerts for anomalies
5. **Dashboards**: Create operational and business dashboards

### Cost Optimization
1. **Right-Sizing**: Start small and scale based on metrics
2. **Reserved Capacity**: Use reserved instances for predictable workloads
3. **Spot Instances**: Use spot/preemptible for non-critical workers
4. **Cache Aggressively**: Reduce redundant LLM calls
5. **Review Filtering**: Skip files unlikely to benefit from review

## Implementation Roadmap - Cloudflare Workers Deployment

### Phase 1: MVP (Days 1-3)
- **Day 1**: Register "ArgoAI" GitHub App name
- **Day 2**: Deploy basic webhook handler to Cloudflare Workers
- **Day 3**: Implement GitHub signature verification
- Simple LLM integration with OpenAI
- Basic comment posting functionality
- Deploy to `api.argoai.dev`

### Phase 2: Production Features (Week 1)
- Implement Cloudflare Queues for async processing
- Add Workers KV caching layer
- Set up D1 database for review history
- Implement rate limiting
- Add structured logging to R2
- Error handling with retries

### Phase 3: Advanced Features (Week 2)
- Multi-model support (OpenAI + Anthropic)
- Custom review rules via KV configuration
- Streaming LLM responses
- Smart comment grouping
- Analytics dashboard with Workers Analytics
- A/B testing framework

### Phase 4: Enterprise & Scale (Week 3-4)
- Multi-repo configuration management
- Team-specific review settings
- Advanced caching strategies
- Custom LLM prompt templates
- Slack/Discord notifications
- API for external integrations

## Deployment Guide

### Quick Start Deployment

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/argoai.git
cd argoai

# 2. Install dependencies
npm install

# 3. Configure Cloudflare account
wrangler login

# 4. Create KV namespaces
wrangler kv:namespace create "CACHE"
wrangler kv:namespace create "RATE_LIMITS"
wrangler kv:namespace create "CONFIG"

# 5. Create Queue
wrangler queues create argoai-reviews

# 6. Set secrets
wrangler secret put GITHUB_APP_PRIVATE_KEY
wrangler secret put GITHUB_WEBHOOK_SECRET
wrangler secret put GITHUB_TOKEN  # GitHub PAT with models:read scope

# 8. Deploy to production
wrangler deploy --env production

# 9. Set up custom domain
# Add CNAME record: api.argoai.dev -> argoai.workers.dev
```

### GitHub App Configuration

1. Go to GitHub Settings â†’ Developer settings â†’ GitHub Apps â†’ ArgoAI
2. Update Webhook URL to: `https://api.argoai.dev/webhooks/github`
3. Set permissions:
   - Pull requests: Read & Write
   - Issues: Write (for comments)
   - Contents: Read
   - Metadata: Read
4. Subscribe to events:
   - Pull request
   - Pull request review
   - Pull request review comment

## Monitoring & Operations

### Cloudflare Dashboard Metrics
- Request volume and latency
- Error rates and types
- Cache hit rates
- Queue depths
- Worker CPU time

### Cloudflare Analytics & Logpush
- Use Cloudflare Dashboard for real-time metrics
- Configure Logpush to send logs to your preferred destination:
  - Amazon S3
  - Google Cloud Storage
  - Datadog
  - Splunk
  - Or any HTTP endpoint

```bash
# Example: Configure Logpush to S3
wrangler logpush create \
  --dataset workers_trace_events \
  --destination "s3://your-bucket/argoai-logs" \
  --fields "timestamp,outcome,scriptName,logs"
```

## Cost Analysis - Cloudflare Workers

### Free Tier Coverage (Most Teams)
- **Workers**: 100,000 requests/day free
- **Workers KV**: 100,000 reads/day free
- **Queues**: 1M messages/month free
- **GitHub Models**: Free tier with GitHub token (rate limited)
- **No storage costs**: Minimal KV usage only

### Paid Tier Estimation
For a team with ~1000 PRs/day:
- **Workers**: ~3M requests/month = $0.45
- **KV Operations**: ~2M reads/month = $1.00
- **Queue Messages**: ~30k/month = $0
- **GitHub Models**: $0 (using free tier)
- **Total**: <$2/month

### Cost Optimization Tips
- GitHub Models free tier handles most workloads
- Cache reviews aggressively to reduce API calls
- Use lightweight models (Phi-3) for simple reviews
- Batch queue processing to reduce invocations

### Cost Comparison
- **ArgoAI (Cloudflare + GitHub Models)**: $0-2/month
- **Traditional LLM Integration**: $50-500/month
- **AWS Lambda + OpenAI**: $100-300/month
- **Kubernetes + Self-hosted**: $500-2000/month

## Conclusion

ArgoAI represents a paradigm shift in code review automation by combining:

1. **GitHub Models Integration** - Free tier LLM access with your GitHub token
2. **Edge-First Architecture** - Cloudflare Workers for zero cold starts
3. **Lightweight Storage** - No unnecessary data retention, just actionable insights
4. **Native Queuing** - Cloudflare Queues for reliable async processing
5. **Minimal Costs** - Often completely free, max $2/month for most teams

### Key Innovations

- **No API Keys Required**: Uses GitHub token for both API and Models access
- **Privacy First**: No persistent storage of code or reviews
- **Global Performance**: Sub-50ms webhook responses worldwide
- **Zero Ops**: No infrastructure to manage or scale

### Getting Started in 15 Minutes

1. **Register "ArgoAI" GitHub App** (5 min)
   - Go to Settings â†’ Developer settings â†’ GitHub Apps â†’ New
   - Name: ArgoAI
   - Webhook URL: `https://api.argoai.dev/webhooks/github` (update later)
   - Permissions: Pull requests (Read & Write), Issues (Write), Contents (Read)

2. **Create GitHub Token for Models** (1 min)
   - Go to Settings â†’ Developer settings â†’ Personal access tokens â†’ Fine-grained tokens
   - Name: ArgoAI Models Access
   - Expiration: 90 days (or custom)
   - Permissions: Only `models:read` required
   - Copy the token (starts with `github_pat_`)

3. **Deploy to Cloudflare Workers** (5 min)
   ```bash
   git clone https://github.com/yourusername/argoai.git
   cd argoai
   npm install
   wrangler login
   wrangler secret put GITHUB_TOKEN
   wrangler deploy --env production
   ```

4. **Configure Webhook URL** (2 min)
   - Return to GitHub App settings
   - Update webhook URL to your Workers URL
   - Save changes

5. **Start reviewing PRs!** (âˆ)
   - Install the app on your repositories
   - Open a PR and watch ArgoAI review it instantly

The combination of GitHub Models and Cloudflare Workers makes enterprise-grade code review automation accessible to everyone - from indie developers to large organizations. No more choosing between quality and cost.

**Start today** - Your code deserves intelligent, instant reviews without the infrastructure overhead.